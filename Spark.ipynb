{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwXzDQyd9B-4",
        "outputId": "1bb1eb26-897c-44f5-cdc5-c10f11cb8f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=d999a1b3b4f0c0becdacf0a3ccd4ee167ac5f32d354498e43c7edfa8d2257a1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "conf = SparkConf().setAppName(\"Assignment3\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "bIOvKA2rY652"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Question 1:**\n",
        "\n"
      ],
      "metadata": {
        "id": "2y-5CffJZCrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.textFile(\"InputQ1.txt\")\n",
        "\n",
        "rdd = rdd.map(lambda x: x.lower())\n",
        "\n",
        "rdd1 = rdd.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "n = rdd1.count()\n",
        "\n",
        "rdd_list = rdd1.collect()\n",
        "\n",
        "consecutive_words = []\n",
        "\n",
        "for i in range(n-2):\n",
        "  if len(rdd_list[i]) > 4 and len(rdd_list[i+1]) > 4 and len(rdd_list[i+2]) > 4 and rdd_list[i][0] == rdd_list[i+1][0] == rdd_list[i+2][0]:\n",
        "    consecutive_words.append((rdd_list[i], rdd_list[i+1], rdd_list[i+2]))\n",
        "\n",
        "print(\"Part A:\")\n",
        "ConsecutiveWordsRdd = sc.parallelize(consecutive_words)\n",
        "total_count = ConsecutiveWordsRdd.count()\n",
        "print(\"Total count:\", total_count)\n",
        "\n",
        "counts_by_alphabet = ConsecutiveWordsRdd.map(lambda x: (x[0][0], 1))\n",
        "counts_by_alphabet = counts_by_alphabet.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "print(\"\\n\\nPart B:\")\n",
        "print(\"Counts by each alphabet:\")\n",
        "for alphabet, count in counts_by_alphabet.collect():\n",
        "    print(alphabet, \"=>\", count)\n",
        "\n",
        "combination_counts = ConsecutiveWordsRdd.map(lambda x: (x, 1))\n",
        "combination_counts = combination_counts.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "print(\"\\n\\nPart C:\")\n",
        "print(\"Combination counts:\")\n",
        "for combination, count in combination_counts.collect():\n",
        "    print(combination, \"=>\", count)"
      ],
      "metadata": {
        "id": "YeP7VB_lZAov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd866b8c-5cc3-46fb-e687-9aec0266b3cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part A:\n",
            "Total count: 6\n",
            "\n",
            "\n",
            "Part B:\n",
            "Counts by each alphabet:\n",
            "h => 3\n",
            "s => 1\n",
            "m => 1\n",
            "f => 1\n",
            "\n",
            "\n",
            "Part C:\n",
            "Combination counts:\n",
            "('massive', 'murree', 'mountains.') => 1\n",
            "('silly', 'stupid', 'samuel’s') => 1\n",
            "('horrid', 'henry’s', 'hound') => 2\n",
            "('henry’s', 'hound', 'hunts') => 1\n",
            "('fantastic', 'fanciful', 'foursome.') => 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:**"
      ],
      "metadata": {
        "id": "JjCr5LMPinmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this function will calculate the simple matching coefficient(SMC) and will return the result\n",
        "def smc(rdd1, rdd2):\n",
        "    return sum([1 for i in range(len(rdd1)) if rdd1[i] == rdd2[i]]) / len(rdd1)"
      ],
      "metadata": {
        "id": "H59hA5iK7YRv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Input1 = sc.textFile(\"Input.txt\")\n",
        "\n",
        "Input2 = sc.textFile(\"Reference.txt\")\n",
        "\n",
        "Inputrdd = Input1.map(lambda line: line.split(\":\")).map(lambda x: (x[0], x[1].split()))\n",
        "\n",
        "Refrdd = Input2.map(lambda line: line.split(\":\")).map(lambda x: (x[0], x[1].split()))\n",
        "\n",
        "PossiblePairRdd = Inputrdd.cartesian(Refrdd)\n",
        "\n",
        "smcRdd = PossiblePairRdd.map(lambda x: (x[0][0], (x[1][0], smc(x[0][1], x[1][1]))))\n",
        "\n",
        "closestRefference = smcRdd.reduceByKey(lambda x, y: x if x[1] >= y[1] else y).sortBy(lambda x: x[0])\n",
        "\n",
        "print(\"Patient ID \\t Closest Reference \\t SMC\")\n",
        "for (PatientID, (RefID, SMCValue)) in closestRefference.collect():\n",
        "    print(PatientID, \"\\t\\t\", RefID, \"\\t\\t\\t\", SMCValue)"
      ],
      "metadata": {
        "id": "p0a78xQUimM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585f60e9-c20f-4b3b-94a7-262b85db235f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient ID \t Closest Reference \t SMC\n",
            "1 \t\t R1 \t\t\t 0.8181818181818182\n",
            "2 \t\t R1 \t\t\t 0.9090909090909091\n",
            "3 \t\t R1 \t\t\t 0.8181818181818182\n",
            "4 \t\t R2 \t\t\t 0.9090909090909091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:**"
      ],
      "metadata": {
        "id": "qHjwVFsnGOhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = sc.textFile(\"InputQ3.txt\")\n",
        "\n",
        "authorsrdd = input_file.map(lambda line: tuple(line.split(\" -> \")))\n",
        "\n",
        "coauthorspair = authorsrdd.flatMap(lambda x: [((x[0], i), x[1].split(\", \")) for i in x[1].split(\", \")])\n",
        "\n",
        "rdd = coauthorspair.map(lambda x: x)\n",
        "authors = []\n",
        "rddList = rdd.collect()\n",
        "n=len(rddList)\n",
        "for i in range(n):\n",
        "  for j in range(i, n-1):\n",
        "    rdd1 = sc.parallelize(rddList[i][0])\n",
        "    rdd2 = sc.parallelize(rddList[j+1][0])\n",
        "    intersection_rdd = rdd1.intersection(rdd2)\n",
        "    if (intersection_rdd.collect() == rdd1.collect()):\n",
        "      rdd3 = sc.parallelize(rddList[i][1])\n",
        "      rdd4 = sc.parallelize(rddList[j+1][1])\n",
        "      rdd5 = rdd3.intersection(rdd4)\n",
        "\n",
        "      authors.append((rddList[i][0], rdd5.collect()))\n",
        "\n",
        "rdd1 = sc.parallelize(authors)\n",
        "rdd1 = rdd1.map(lambda x: (x[0], len(x[1]), x[1]))\n",
        "rdd1.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ637rhN20oi",
        "outputId": "54b922bc-e3eb-4ce6-89ea-e4b4cead8f23"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('Y. Lu', 'B. Cao'), 3, ['F. Glover', 'C. Rego', 'K. Kiani']),\n",
              " (('Y. Lu', 'C. Rego'), 1, ['B. Cao']),\n",
              " (('Y. Lu', 'F. Glover'), 1, ['B. Cao']),\n",
              " (('Y. Lu', 'K. Kiani'), 1, ['B. Cao']),\n",
              " (('B. Cao', 'C. Rego'), 1, ['Y. Lu']),\n",
              " (('B. Cao', 'K. Kiani'), 1, ['Y. Lu']),\n",
              " (('B. Hosseini', 'K. Kiani'), 0, [])]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}